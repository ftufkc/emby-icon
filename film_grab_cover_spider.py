#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
film_grab_cover_spider.py
~~~~~~~~~~~~~~~~~~~~~~~~~
æŠ“å– https://film-grab.com/ 1-62 é¡µä¸­çš„å°é¢å›¾ç‰‡ï¼ˆä»…ä¸‹è½½ <img> çš„ srcï¼‰ã€‚
"""

import os
import time
import pathlib
import logging
from urllib.parse import urlparse
import requests
from bs4 import BeautifulSoup

# =============== é…ç½®é¡¹ ===============
BASE_URL   = "https://film-grab.com"
PAGES      = range(2, 63)      # 1-62
PAUSE_SEC  = 1.0
SAVE_DIR   = pathlib.Path("film_grab_covers")
UA         = ("Mozilla/5.0 (Windows NT 10.0; Win64; x64) "
              "AppleWebKit/537.36 (KHTML, like Gecko) "
              "Chrome/125.0.0.0 Safari/537.36")

# =============== å·¥å…·å‡½æ•° ===============
def ensure_dir(path: pathlib.Path) -> None:
    path.mkdir(parents=True, exist_ok=True)

### æ”¹åŠ¨ 1ï¼šåªå– srcï¼ˆä¸å†è§£æ srcset / data-*ï¼‰
def get_img_src(img_tag) -> str | None:
    src = img_tag.get("src")
    if not src:
        return None
    if src.startswith("//"):
        return "https:" + src
    if src.startswith("/"):
        return BASE_URL + src
    return src

def download(url: str, dest: pathlib.Path) -> bool:
    if dest.exists():
        return False
    try:
        r = requests.get(url, headers={"User-Agent": UA}, timeout=15)
        r.raise_for_status()
    except Exception as e:
        logging.warning("fail %s â€“ %s", url, e)
        return False
    dest.write_bytes(r.content)
    logging.info("saved %s", dest.name)
    return True

def crawl_page(n: int) -> None:
    page_url = BASE_URL if n == 1 else f"{BASE_URL}/page/{n}/"
    logging.info(">>> Page %d", n)

    try:
        r = requests.get(page_url, headers={"User-Agent": UA}, timeout=15)
        r.raise_for_status()
    except Exception as e:
        logging.error("request %s â€“ %s", page_url, e)
        return

    soup = BeautifulSoup(r.text, "html.parser")

    ### æ”¹åŠ¨ 2ï¼šç²¾ç¡®åœ¨ <article> ä¸­æ‰¾ç¬¬ä¸€å¼  <img>ï¼ˆwp-post-imageï¼‰
    for art in soup.select("article"):
        img = art.select_one("img.wp-post-image") or art.find("img")
        if not img:
            continue
        img_url = get_img_src(img)
        if not img_url:
            continue

        filename = os.path.basename(urlparse(img_url).path)
        download(img_url, SAVE_DIR / filename)

# =============== ä¸»å…¥å£ ===============
def main():
    logging.basicConfig(level=logging.INFO,
                        format="%(levelname)s: %(message)s")
    ensure_dir(SAVE_DIR)
    for p in PAGES:
        crawl_page(p)
        time.sleep(PAUSE_SEC)
    logging.info("ğŸ¬ Done â€” images in %s", SAVE_DIR.resolve())

if __name__ == "__main__":
    main()
